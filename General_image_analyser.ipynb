{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DB2_ZSJVR3k",
        "outputId": "f357ba49-d690-43ed-b863-3551d3e7daff"
      },
      "outputs": [],
      "source": [
        "%pip install fastapi==0.104.1 uvicorn[standard]==0.24.0 python-multipart==0.0.6 langchain==0.1.9 unstructured[all-docs]==0.11.2 sentence-transformers==2.2.2 llama-index==0.9.22 dataclass-wizard==0.22.2 opencv-python==4.8.0.74 llama-hub==0.0.43 pymilvus==2.3.1 jupyterlab==4.0.8 langchain-core==0.1.29 langchain-nvidia-ai-endpoints==0.1.1 atlassian-python-api==3.41.4 gradio==3.48.0 markdownify==0.12.1 scikit-image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Export the NVIDIA_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAafaGASV0i0",
        "outputId": "ec2145c0-8f8c-419d-a790-1871e60f28fa"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
        "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
        "else:\n",
        "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
        "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
        "global nvapi_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Set 'ai-mixtral_8x7b-instruct' model as LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MTLN-nFZody"
      },
      "outputs": [],
      "source": [
        "#Set up Prerequisites for Image Captioning App User Interface\n",
        "import os\n",
        "import io\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "import gradio as gr\n",
        "import openai, httpx, sys\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlxAkmB_3MSK",
        "outputId": "dd5329be-d22b-46cc-9d5c-ed47d21d53d6"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "llm = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\", nvidia_api_key=nvapi_key, max_tokens=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Wrap 'fuyu-8b' model in a tool for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkBiOkDNZsCN"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import BaseTool\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
        "import torch\n",
        "from tempfile import NamedTemporaryFile\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "def fetch_outputs(output):\n",
        "    collect_streaming_outputs=[]\n",
        "    for o in output:\n",
        "        try:\n",
        "            start = o.index('{')\n",
        "            jsonString=o[start:]\n",
        "            d = json.loads(jsonString)\n",
        "            temp=d['choices'][0]['delta']['content']\n",
        "            collect_streaming_outputs.append(temp)\n",
        "        except:\n",
        "            pass\n",
        "    outputs=''.join(collect_streaming_outputs)\n",
        "    return outputs.replace('\\\\','').replace('\\'','')\n",
        "\n",
        "def img2base64_string(img_path):\n",
        "    image = Image.open(img_path)\n",
        "    if image.width > 800 or image.height > 800:\n",
        "        image.thumbnail((800, 800))\n",
        "    buffered = io.BytesIO()\n",
        "    image.convert(\"RGB\").save(buffered, format=\"JPEG\", quality=85)\n",
        "    image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "    return image_base64\n",
        "\n",
        "\n",
        "class ImageCaptionTool(BaseTool):\n",
        "    name = \"Image analyser from Fuyu\"\n",
        "    description = \"Use this tool when given the path to an image and a query about the image that you would like answered. \" \\\n",
        "                  \"It will return a caption responding to the query about the image.\"\n",
        "\n",
        "    def _run(self, combined_input):\n",
        "        print(\"Running ImageCaptionTool\")\n",
        "        print(combined_input)\n",
        "        img_path, user_prompt = combined_input.split(\"###\", 1)\n",
        "        #img_path = './sample_data/sheep.png'\n",
        "        #user_prompt = 'how many sheep in this image?'\n",
        "        print(img_path)\n",
        "        print(user_prompt)\n",
        "        invoke_url = \"https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b\"\n",
        "        stream = True\n",
        "\n",
        "\n",
        "        image_b64=img2base64_string(img_path)\n",
        "\n",
        "\n",
        "        assert len(image_b64) < 200_000, \\\n",
        "          \"To upload larger images, use the assets API (see docs)\"\n",
        "        headers = {\n",
        "          \"Authorization\": f\"Bearer {nvapi_key}\",\n",
        "          \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "          \"messages\": [\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f'Given the following image and question, answer the question using the contents of the image\". <img src=\"data:image/png;base64,{image_b64}\" />. This is the question: {user_prompt}'\n",
        "            }\n",
        "          ],\n",
        "          \"max_tokens\": 1024,\n",
        "          \"temperature\": 0.20,\n",
        "          \"top_p\": 0.70,\n",
        "          \"seed\": 0,\n",
        "          \"stream\": stream\n",
        "        }\n",
        "\n",
        "        response = requests.post(invoke_url, headers=headers, json=payload)\n",
        "\n",
        "        if stream:\n",
        "            output=[]\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    output.append(line.decode(\"utf-8\"))\n",
        "        else:\n",
        "            output=response.json()\n",
        "        out=fetch_outputs(output)\n",
        "        return out\n",
        "\n",
        "    def _arun(self, query: str):\n",
        "        raise NotImplementedError(\"This tool does not support async\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Initialize our LangChain agent with the ImageCaptionTool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Y_qgJLZxCQ"
      },
      "outputs": [],
      "source": [
        "#initialize the agent\n",
        "tools = [ImageCaptionTool()]\n",
        "\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    max_iterations=5,\n",
        "    verbose=True,\n",
        "    memory=conversational_memory,\n",
        "    handle_parsing_errors=True,\n",
        "    early_stopping_method='generate'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcura8G2Z6P9"
      },
      "outputs": [],
      "source": [
        "def my_agent(img_path, user_prompt):\n",
        "    combined_input = f\"{img_path}###{user_prompt}\"\n",
        "    response = agent.invoke({\"input\":f'Use the ImageCaptionTool with this entire input: {combined_input}'})\n",
        "    return response['output']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5b (optional): Verify that the agent works with this test example "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGQv2SdHb1zD",
        "outputId": "63e39e18-9895-4a8d-b4ea-3fef1438e43c"
      },
      "outputs": [],
      "source": [
        "!wget \"https://media.istockphoto.com/id/182344013/photo/sheep.jpg?s=1024x1024&w=is&k=20&c=c9qHEcNRispCkFu-pITLA-LGYvlMSNt8igmOMVz10mo=\" -O ./sample_data/sheep.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "ie_FBkxEbvHO",
        "outputId": "a007df45-4c8a-4868-99ba-25bdfad10b52"
      },
      "outputs": [],
      "source": [
        "my_agent('./sample_data/sheep.png', 'what is the animal in this image?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Wrap the agent in a Gradio UI for easy interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "QzZJ8guEZ8iF",
        "outputId": "105c466d-cf8c-4cdb-89c8-f7e3894c1bb5"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "ImageCaptionApp = gr.Interface(fn=my_agent,\n",
        "                    inputs=[gr.Image(label=\"Upload image\", type=\"filepath\"), gr.Textbox(label=\"Enter a question about the image\")],\n",
        "                    outputs=[gr.Textbox(label=\"Response\")],\n",
        "                    title=\"Image Analysis using Nvidia API\",\n",
        "                    description=\"Upload an image and a query to find out more about the image\",\n",
        "                    debug=True,\n",
        "                    allow_flagging=\"never\")\n",
        "\n",
        "ImageCaptionApp.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
